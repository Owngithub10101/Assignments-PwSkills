{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJS7sBYbUyIO"
      },
      "source": [
        "# Assignment : 20(21th Feb'2023)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sn4pyiX7Rghn"
      },
      "source": [
        "1. Web scraping is the process of extracting data from websites automatically, using software tools called **web scrapers or web crawlers**. Web scraping involves parsing the HTML or other markup language of a web page and extracting useful information, such as text, images, links, and metadata.\n",
        "\n",
        "    Web scraping is used for various purposes, including data mining, market research, competitor analysis, price monitoring, and content aggregation. By scraping data from multiple sources, organizations can gain valuable insights into consumer behavior, market trends, and industry dynamics.\n",
        "\n",
        "    Here are three areas where web scraping is commonly used :\n",
        "\n",
        "  * **E-commerce :** Web scraping is used by e-commerce businesses to monitor prices, track product availability, and gather information about competitors. For example, an e-commerce company may use web scraping to track prices for a specific product on different websites and adjust its prices accordingly.\n",
        "\n",
        "  *  **Research :** Researchers in various fields, including social sciences, medicine, and economics, use web scraping to collect data for their studies. For example, a social scientist may use web scraping to collect data on social media platforms to study online behavior and trends.\n",
        "\n",
        "  * **Finance :** Web scraping is used in the finance industry to gather data on stock prices, market trends, and economic indicators. For example, a financial analyst may use web scraping to monitor news websites and social media platforms to identify emerging trends and sentiment that could impact financial markets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xtsIP-SCCy"
      },
      "source": [
        "2. Here are some of the most common methods :\n",
        "\n",
        "* **Manual Scraping :** This is the most basic method of web scraping, where a person manually copies and pastes data from web pages into a spreadsheet or text editor. Manual scraping is time-consuming and not suitable for large-scale data extraction, but it can be useful for small projects or one-time data collection.\n",
        "\n",
        "* **Web Scraping Tools :** There are many web scraping tools available that can automate the scraping process. These tools typically use a web browser or API to extract data from web pages and store it in a structured format such as CSV, JSON, or XML. Popular web scraping tools include BeautifulSoup, Scrapy, Selenium, and Puppeteer.\n",
        "\n",
        "* **APIs :** Many websites offer APIs (Application Programming Interfaces) that allow developers to access data programmatically. APIs provide a structured way to retrieve data, making them more reliable and efficient than web scraping. However, not all websites offer APIs, and those that do may require authentication or charge fees for access.\n",
        "\n",
        "* **DOM Parsing :** DOM (Document Object Model) parsing is a technique that involves extracting data from the HTML source code of a web page. DOM parsing can be faster than other scraping methods, but it requires knowledge of HTML and JavaScript.\n",
        "* **Headless Browsers :** Headless browsers are web browsers that can run without a graphical user interface. They can be used for web scraping because they can simulate user behavior and interact with web pages programmatically. Headless browsers are useful for scraping websites that require user authentication or use JavaScript to load data dynamically.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn-2XHYUSnzn"
      },
      "source": [
        "3. Beautiful Soup is a Python library used for web scraping purposes to extract specific data from a web page, such as text, links, images, and tables.\n",
        "\n",
        "    It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2dMHwwS22h"
      },
      "source": [
        "4. Flask is a lightweight Python web framework that is commonly used for create a RESTful API that enables users to interact with the web scraping script.\n",
        "\n",
        "    Using Flask to create a RESTful API provides several benefits for this Web Scraping project, including:\n",
        "\n",
        "  * **Ease of use :** Flask is a lightweight and easy-to-use web application framework that makes it simple to create APIs in Python.\n",
        "\n",
        "  * **Flexibility :** Flask provides a flexible way to define API endpoints, making it easy to create and modify endpoints to suit the needs of the project.\n",
        "\n",
        "  * **Scalability :** Flask is well-suited for creating scalable web applications and APIs, making it a good choice for a project like web scraping where there may be a large number of requests to handle.\n",
        "\n",
        "  * **Integration :** Flask can be easily integrated with other libraries and tools in the Python ecosystem, such as Beautiful Soup and Requests, which are commonly used for web scraping.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GxcqDUwGT_jV"
      },
      "source": [
        "5. * **Code-Pipline :** AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n",
        "\n",
        "    * **Bean-Stalk :** AWS Elastic Beanstalk automates the details of capacity provisioning, load balancing, auto scaling, and application deployment, creating an environment that runs a version of your application. \n",
        "\n",
        "    **For Example :**\n",
        "    \n",
        "    <img src=\"picture.png\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
